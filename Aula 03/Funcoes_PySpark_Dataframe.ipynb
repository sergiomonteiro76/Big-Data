{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Explorar as Funcionalidades do PySpark**\n",
        "\n",
        "**prof: Sergio Assuncao Monteiro, DSc**\n",
        "\n",
        "lattes: http://lattes.cnpq.br/9489191035734025"
      ],
      "metadata": {
        "id": "w0yzKN6kJOCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(0) Pre-Requisitos**"
      ],
      "metadata": {
        "id": "9jW2OtrGJh4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRhZ1htBJGlV"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(1) Configuracao das Variaveis de Ambiente**"
      ],
      "metadata": {
        "id": "qceeo_5GJlw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "CqdKhCfBJyCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exemplo 01:**"
      ],
      "metadata": {
        "id": "bheOTXTbJ7ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "l5d7DA9KK2SB",
        "outputId": "564343bd-7051-4814-eb42-699e4e399fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79d328747730>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1304f76f2722:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Spark Dataframe\n",
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "uIP4iQvINUtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7C9lZTZNXvl",
        "outputId": "58a468d1-19ab-4e58-aa68-9835c14808d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ahew1nNbsV",
        "outputId": "eaca657b-705e-43de-8055-364e2591ac4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0),\n",
              " Row(longitude=-118.3, latitude=34.26, housing_median_age=43.0, total_rooms=1510.0, total_bedrooms=310.0, population=809.0, households=277.0, median_income=3.599, median_house_value=176500.0),\n",
              " Row(longitude=-117.81, latitude=33.78, housing_median_age=27.0, total_rooms=3589.0, total_bedrooms=507.0, population=1484.0, households=495.0, median_income=5.7934, median_house_value=270500.0),\n",
              " Row(longitude=-118.36, latitude=33.82, housing_median_age=28.0, total_rooms=67.0, total_bedrooms=15.0, population=49.0, households=11.0, median_income=6.1359, median_house_value=330000.0),\n",
              " Row(longitude=-119.67, latitude=36.33, housing_median_age=19.0, total_rooms=1241.0, total_bedrooms=244.0, population=850.0, households=237.0, median_income=2.9375, median_house_value=81700.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPYdH5fKNfXl",
        "outputId": "94a9f9e4-8a52-4e06-ce02-881253872464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a temporary SQL Table\n",
        "dataset.createOrReplaceTempView(\"tabela_temporaria\")\n",
        "print(spark.catalog.listTables()) # Print the tables in the catalog"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAdIOoGFNkPV",
        "outputId": "1854d8a8-49bf-4d21-fa31-218d5afc7603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query = \"FROM tabela_temporaria SELECT * LIMIT 3\"  # Don't change this query\n",
        "query = \"FROM tabela_temporaria SELECT longitude, latitude LIMIT 3\"  # Don't change this query\n",
        "\n",
        "saida = spark.sql(query)  # Get the first 10 rows of flights\n",
        "saida.show() # Show the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNP7Egh7NoV1",
        "outputId": "bdf02b80-569e-49f9-ad17-b07b8ba4a593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converter SQL para PANDAS**"
      ],
      "metadata": {
        "id": "CN7yjtdNNt9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't change this query\n",
        "query1 = \"SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria\"\n",
        "\n",
        "q_maximo_quartos = spark.sql(query1)\n",
        "\n",
        "pd_maximo_quartos = q_maximo_quartos.toPandas()\n",
        "\n",
        "print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos['maximo_quartos']))\n",
        "\n",
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0,'maximo_quartos'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVk7QILBNu2G",
        "outputId": "4c1740fe-937a-4b7f-c1fb-7b6ad23ae318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é: 0    30450.0\n",
            "Name: maximo_quartos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query no Spark**"
      ],
      "metadata": {
        "id": "uzshWkODNzpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms = \"+str(qtd_maximo_quartos)\n",
        "localizacao_maximo_quartos = spark.sql(query2)                      # Run the query\n",
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas() # Convert the results to a pandas DataFrame\n",
        "print(pd_localizacao_maximo_quartos.head())                         # Print the head of pd_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKhg79kBNz_u",
        "outputId": "e25fccfe-6ad5-45ea-f21a-570dfc8f5cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converter PANDAS DataFrame para Spark DataFrame**"
      ],
      "metadata": {
        "id": "jzOMwQ2qN3EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "media = 0\n",
        "desvio_padrao=0.1\n",
        "pd_temporario = pd.DataFrame(np.random.normal(media,desvio_padrao,100))                 # Create pd_temp\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)             # Create spark_temp from pd_temp\n",
        "print(spark.catalog.listTables())                                   # Examine the tables in the catalog\n",
        "spark_temporario.createOrReplaceTempView(\"nova_tabela_temporaria\")  # Add spark_temp to the catalog\n",
        "print(spark.catalog.listTables())                                   # Examine the tables in the catalog again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TEHWvojN351",
        "outputId": "0a8b942c-175b-4154-bfa2-342744b12bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "pZvVv1ebN-gG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}